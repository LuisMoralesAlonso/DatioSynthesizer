{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from math import log\n",
    "from collections import Counter\n",
    "import functools \n",
    "import sklearn.metrics as metrics\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[0 1 2 2 1 0]\n",
      "0.4620981203732969\n",
      "[4 5]\n",
      "[0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "trues = np.array([1,2,3,3,2,1])\n",
    "preds = np.array([4,5,4,4,5,5])\n",
    "true_classes, true_idx =np.unique(trues, return_inverse=True)\n",
    "print(true_classes)\n",
    "print(true_idx)\n",
    "pred_classes, pred_idx =np.unique(preds, return_inverse=True)\n",
    "print(metrics.mutual_info_score(trues, preds))\n",
    "print(pred_classes)\n",
    "print(pred_idx)\n",
    "n_classes = true_classes.shape[0]\n",
    "n_preds = pred_classes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4620981203732969\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def partition_mutual_info_pre_score(true: pd.Series, pred: pd.Series):\n",
    "    datos = {}\n",
    "    true_classes, true_idx = np.unique(true, return_inverse=True)\n",
    "    datos['true_classes'] = true_classes\n",
    "    datos['true_idx'] = true_idx\n",
    "    pred_classes, pred_idx = np.unique(pred, return_inverse=True)\n",
    "    datos['pred_classes'] = pred_classes\n",
    "    datos['pred_idx'] = pred_idx\n",
    "    n_classes = true_classes.shape[0]\n",
    "    n_preds = pred_classes.shape[0]\n",
    "    datos['n_classes'] = n_classes\n",
    "    datos['n_preds'] = n_preds\n",
    "    contingency = sp.coo_matrix((np.ones(true_idx.shape[0]),\n",
    "                                 (true_idx, pred_idx)),\n",
    "                                shape=(n_classes, n_preds),\n",
    "                                dtype=np.int)\n",
    "    nzx, nzy, nz_val = sp.find(contingency)\n",
    "    datos['nzx'], datos['nzy'], datos['nz_val'] = nzx, nzy, nz_val\n",
    "    contingency_sum = contingency.sum()\n",
    "    datos['contingency_sum'] = contingency_sum\n",
    "    pi = np.ravel(contingency.sum(axis=1))\n",
    "    datos['pi'] = pi\n",
    "    pj = np.ravel(contingency.sum(axis=0))\n",
    "    datos['pj'] = pj\n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(nout=2)\n",
    "def gen_pi_pj(chunks_mi_list: list, true_classes: list, pred_classes: list):\n",
    "    #pi_dask = [0 for i in range(true_classes_len)]\n",
    "    pi_dask = np.zeros(len(true_classes))\n",
    "    pj_dask = np.zeros(len(pred_classes))\n",
    "    for mi_chunk in chunks_mi_list:\n",
    "        for index, clase in enumerate(true_classes):\n",
    "            try:\n",
    "                index_true_clase = mi_chunk['true_classes'].tolist().index(clase)\n",
    "                pi_dask[index] = pi_dask[index] + mi_chunk['pi'][mi_chunk['true_classes'].tolist().index(clase)]\n",
    "            except (IndexError, ValueError):\n",
    "                None\n",
    "        for index, clase in enumerate(pred_classes):\n",
    "            try:\n",
    "                index_pred_clase = mi_chunk['pred_classes'].tolist().index(clase)\n",
    "                pj_dask[index] = pj_dask[index] + mi_chunk['pj'][mi_chunk['pred_classes'].tolist().index(clase)]\n",
    "            except (IndexError, ValueError):\n",
    "                None\n",
    "    return (pi_dask, pj_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(nout=3)\n",
    "def gen_nzx_nzy_nzval_dask(chunks_mi_list: list, true_classes, pred_classes):\n",
    "    nzx_dask, nzy_dask, nz_val_dask = np.array([], dtype=np.int64),np.array([], dtype=np.int64),np.array([], dtype=np.int64)\n",
    "    cross_clusters_list = []\n",
    "    for mi_chunk in chunks_mi_list:\n",
    "        true_nzx_np = np.array(list(map(lambda x: mi_chunk['true_classes'][x], mi_chunk['nzx'])))\n",
    "        true_nzy_np = np.array(list(map(lambda x: mi_chunk['pred_classes'][x], mi_chunk['nzy'])))\n",
    "        true_nz_val = mi_chunk['nz_val']\n",
    "        cross_clusters_list.append(Counter(dict(list(zip(zip(true_nzx_np,true_nzy_np),true_nz_val)))))\n",
    "    cross_clusters = dict(functools.reduce(lambda a,b : a+b,cross_clusters_list))\n",
    "    for key in cross_clusters.keys():\n",
    "        nzx_dask = np.append(nzx_dask, true_classes.tolist().index(key[0]))\n",
    "        nzy_dask = np.append(nzy_dask, pred_classes.tolist().index(key[1]))\n",
    "        nz_val_dask = np.append(nz_val_dask, cross_clusters[key])\n",
    "    return (nzx_dask, nzy_dask, nz_val_dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def mutual_info_score(chunked_mi_list: list, trues: df.Series, preds: df.Series):\n",
    "    pi, pj = gen_pi_pj(chunked_mi_list, trues, preds)\n",
    "    nzx,nzy,nz_val = gen_nzx_nzy_nzval_dask(chunked_mi_list)\n",
    "    contingency_sum = get_contingency_sum(chunked_mi_list)\n",
    "    log_contingency_nm = np.log(nz_val)\n",
    "    print(log_contingency_nm)\n",
    "    contingency_nm = nz_val / contingency_sum\n",
    "    print(contingency_nm)\n",
    "    # Don't need to calculate the full outer product, just for non-zeroes\n",
    "    outer = pi.take(nzx).astype(np.int64) * pj.take(nzy).astype(np.int64)\n",
    "    print(pi.take(nzx).astype(np.int64))\n",
    "    print(outer)\n",
    "    print(pj.take(nzy).astype(np.int64))\n",
    "    log_outer = get_log_outer(outer,pi,pj)\n",
    "    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n",
    "          contingency_nm * log_outer)\n",
    "    return mi.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def get_log_outer(outer_delayed, pi_delayed, pj_delayed):\n",
    "    print(outer_delayed)\n",
    "    print(pi_delayed)\n",
    "    print(pj_delayed)\n",
    "    return -np.log(outer_delayed) + np.log(sum(pi_delayed)) + np.log(sum(pj_delayed))\n",
    "\n",
    "@dask.delayed\n",
    "def get_mi(contingency_nm_d, log_contingency_nm_d, contingency_sum_d, log_outer_d):\n",
    "    return (contingency_nm_d * (log_contingency_nm_d - log(contingency_sum_d)) +\n",
    "          contingency_nm_d * log_outer_d)\n",
    "\n",
    "@dask.delayed\n",
    "def get_contingency_sum(chunks_mi_list: list):\n",
    "    suma = 0\n",
    "    for mi_chunk in chunks_mi_list:\n",
    "        suma = suma + mi_chunk['contingency_sum']\n",
    "    return suma\n",
    "\n",
    "@dask.delayed\n",
    "def get_log_contingency_nm(nz_val_delayed):\n",
    "    return np.log(nz_val_delayed)\n",
    "\n",
    "@dask.delayed\n",
    "def get_contingency_nm(contingency_sum_delayed, nz_val_delayed):\n",
    "    contingency_nm = nz_val_delayed / contingency_sum_delayed\n",
    "    return contingency_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(true: df.DataFrame, pred: df.DataFrame):\n",
    "    #Mutual information of distributions in format of pd.Series or pd.DataFrame.\n",
    "    str_trues = true.astype(str).apply(lambda x: ' '.join(x.tolist()), axis=1, meta=('phrase', 'object'))\n",
    "    str_preds = pred.astype(str).apply(lambda x: ' '.join(x.tolist()), axis=1, meta=('phrase', 'object'))\n",
    "    true_classes = str_trues.unique()\n",
    "    pred_classes = str_preds.unique()\n",
    "    chunked_mi_list = list(map(lambda x: partition_mutual_info_pre_score(x[0],x[1]),list(zip(str_trues.to_delayed(), \n",
    "                                                                       str_preds.to_delayed()))))\n",
    "    pi, pj = gen_pi_pj(chunked_mi_list, true_classes, preds)\n",
    "    nzx,nzy,nz_val = gen_nzx_nzy_nzval_dask(chunked_mi_list, true_classes, pred_classes)\n",
    "    contingency_sum = get_contingency_sum(chunked_mi_list)\n",
    "    log_contingency_nm = get_log_contingency_nm(nz_val)\n",
    "    contingency_nm = get_contingency_nm(chunked_mi_list, nz_val)\n",
    "    \n",
    "    # Don't need to calculate the full outer product, just for non-zeroes\n",
    "    outer = pi.take(nzx).astype(np.int64) * pj.take(nzy).astype(np.int64)\n",
    "    log_outer = get_log_outer(outer, pi, pj)\n",
    "    mi = get_mi(contingency_nm, log_contingency_nm, contingency_sum, log_outer)\n",
    "    return mi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = df.from_pandas(pd.DataFrame([1,2,3,3,2,1]), npartitions=2)\n",
    "preds = df.from_pandas(pd.DataFrame([4,5,4,4,5,5]), npartitions=2)\n",
    "mi = mutual_information(trues, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_bayes(dataset: df.DataFrame, k=0: int, epsilon=0: float):\n",
    "    \"\"\"Construct a Bayesian Network (BN) using greedy algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset : DataFrame\n",
    "            Input dataset, which only contains categorical attributes.\n",
    "        k : int\n",
    "            Maximum degree of the constructed BN. If k=0, k is automatically calculated.\n",
    "        epsilon : float\n",
    "            Parameter of differential privacy.\n",
    "    \"\"\"\n",
    "\n",
    "    num_tuples, num_attributes = dataset.shape\n",
    "    if not k:\n",
    "        k = calculate_k(num_attributes, num_tuples)\n",
    "\n",
    "    attributes = set(dataset.keys())\n",
    "    N = []\n",
    "    V = set()\n",
    "    V.add(random.choice(attributes))\n",
    "\n",
    "    print('================== Constructing Bayesian Network ==================')\n",
    "    for i in range(1, len(attributes)):\n",
    "        print('Looking for next attribute-parents pair.')\n",
    "        rest_attributes = attributes - V\n",
    "        parents_pair_list = []\n",
    "        mutual_info_list = []\n",
    "        for child in rest_attributes:\n",
    "            print('    Considering attribute {}'.format(child))\n",
    "            for parents in combinations(V, min(k, len(V))):\n",
    "                parents = list(parents)\n",
    "                parents_pair_list.append((child, parents))\n",
    "                # TODO consider to change the computation of MI by combined integers instead of strings.\n",
    "                mi = mutual_information(dataset[child], dataset[parents])\n",
    "                mutual_info_list.append(mi)\n",
    "\n",
    "        if epsilon:\n",
    "            sampling_distribution = exponential_mechanism(dataset, mutual_info_list, epsilon)\n",
    "            idx = np.random.choice(list(range(len(mutual_info_list))), p=sampling_distribution)\n",
    "        else:\n",
    "            idx = mutual_info_list.index(max(mutual_info_list))\n",
    "\n",
    "        N.append(parents_pair_list[idx])\n",
    "        V.add(parents_pair_list[idx][0])\n",
    "\n",
    "    print('========================= BN constructed =========================')\n",
    "\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.from_pandas(pd.DataFrame(data={'col1':[1,2,3,3,2,1], 'col2':[4,5,4,4,5,5], 'col3':[3,4,3,4,3,4],\n",
    "                                            'col4':[1,2,3,4,5,6], 'col5':[3,3,3,3,3,3]}), npartitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['col1', 'col2', 'col3', 'col4', 'col5'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_network = [('col1',['col3']),('col4',['col1', 'col3']),\n",
    "                    ('col2',['col1','col4']),('col5',['col1','col4'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-9b25bbe5c955>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-9b25bbe5c955>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    bayesian_network[:2]:\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bayesian_network[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: getitem, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                col1   col2\n",
       "npartitions=2              \n",
       "0              int64  int64\n",
       "3                ...    ...\n",
       "5                ...    ...\n",
       "Dask Name: getitem, 4 tasks"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['col1','col2']\n",
    "dataset.copy()[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = data.groupby(attributes).agg('sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           col3  col4  col5\n",
      "col1 col2                  \n",
      "1    4        3     1     3\n",
      "     5        4     6     3\n",
      "2    5        7     7     6\n",
      "3    4        7     7     6\n"
     ]
    }
   ],
   "source": [
    "print(stats.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.from_pandas(pd.DataFrame(data={'col1':[1,2,3,3,2,1], 'col2':[4,5,4,4,5,5], 'col3':[3,4,3,4,3,4],\n",
    "                                            'col4':[1,2,3,4,5,6], 'col5':[3,3,3,3,3,3], 'cols':['a','b','c','d','e','f']}), npartitions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cols.map(len, meta=('len', int)).min().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dasaset2 = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2=dataset.drop('col1',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<truediv..., dtype=float64>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.size / dasaset2.col3.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=dataset2.col2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    int64\n",
       "      ...\n",
       "Name: col2, dtype: int64\n",
       "Dask Name: clip, 10 tasks"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.clip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<eq-0b7f..., dtype=bool>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
